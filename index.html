<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="https://websim.ai" />
    <title>Humanistic AI Assistant - Real-time Voice Interaction</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #1a2a6c, #b21f1f, #fdbb2d);
            margin: 0;
            height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            overflow: hidden;
        }
        .assistant-container {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.37);
            backdrop-filter: blur(4px);
            border: 1px solid rgba(255, 255, 255, 0.18);
            width: 80%;
            max-width: 800px;
            text-align: center;
        }
        h1 {
            color: #ffffff;
            font-size: 2.5em;
            margin-bottom: 20px;
        }
        .avatar {
            width: 200px;
            height: 200px;
            margin: 0 auto 20px;
            position: relative;
        }
        .avatar-face {
            width: 100%;
            height: 100%;
            border-radius: 50%;
            background: #f0f0f0;
            position: relative;
            overflow: hidden;
        }
        .eye {
            width: 30px;
            height: 30px;
            background: #333;
            border-radius: 50%;
            position: absolute;
            top: 40px;
            transition: all 0.3s ease;
        }
        .eye.left {
            left: 40px;
        }
        .eye.right {
            right: 40px;
        }
        .mouth {
            width: 80px;
            height: 40px;
            border-radius: 0 0 40px 40px;
            border: 10px solid #333;
            border-top: none;
            position: absolute;
            bottom: 40px;
            left: 50%;
            transform: translateX(-50%);
            transition: all 0.3s ease;
        }
        .response {
            color: #ffffff;
            font-size: 1.2em;
            margin-top: 20px;
            height: 100px;
            overflow-y: auto;
        }
        #visualizer {
            width: 100%;
            height: 50px;
            margin-top: 20px;
        }
    </style>
</head>
<body>
    <div class="assistant-container">
        <h1>Humanistic AI Assistant</h1>
        <div class="avatar">
            <div class="avatar-face">
                <div class="eye left"></div>
                <div class="eye right"></div>
                <div class="mouth"></div>
            </div>
        </div>
        <div class="response" id="response">
            Hello! I'm your AI Assistant. Just start speaking, and I'll listen.
        </div>
        <canvas id="visualizer"></canvas>
    </div>

    <script>
    document.addEventListener('DOMContentLoaded', (event) => {
        const responseDiv = document.getElementById('response');
        const leftEye = document.querySelector('.eye.left');
        const rightEye = document.querySelector('.eye.right');
        const mouth = document.querySelector('.mouth');
        const visualizer = document.getElementById('visualizer');
        const visualizerContext = visualizer.getContext('2d');

        let isListening = false;
        let audioContext;
        let analyser;
        let microphone;
        let javascriptNode;
        let recognition;
        let userId = 'user123';
        let mouthInterval;
        let inactivityTimeout;
        let lastSpeechTimestamp = Date.now();
        let isAssistantSpeaking = false;
        let currentUtterance = null;

        function setupAudioContext() {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            javascriptNode = audioContext.createScriptProcessor(2048, 1, 1);

            analyser.smoothingTimeConstant = 0.8;
            analyser.fftSize = 1024;

            javascriptNode.connect(audioContext.destination);
        }

        function animateAvatar() {
            // Eye movement
            setInterval(() => {
                const maxMove = 5;
                const leftX = Math.random() * maxMove - maxMove / 2;
                const leftY = Math.random() * maxMove - maxMove / 2;
                const rightX = Math.random() * maxMove - maxMove / 2;
                const rightY = Math.random() * maxMove - maxMove / 2;

                leftEye.style.transform = `translate(${leftX}px, ${leftY}px)`;
                rightEye.style.transform = `translate(${rightX}px, ${rightY}px)`;
            }, 2000);

            // Blinking
            setInterval(() => {
                leftEye.style.height = '0px';
                rightEye.style.height = '0px';
                setTimeout(() => {
                    leftEye.style.height = '30px';
                    rightEye.style.height = '30px';
                }, 200);
            }, 5000);
        }

        function animateMouth(talking) {
            if (talking) {
                clearInterval(mouthInterval);
                let open = true;
                mouthInterval = setInterval(() => {
                    mouth.style.height = open ? '20px' : '40px';
                    open = !open;
                }, 200);
            } else {
                clearInterval(mouthInterval);
                mouth.style.height = '40px';
            }
        }

        function startListening() {
            if (!audioContext) setupAudioContext();

            navigator.mediaDevices.getUserMedia({ audio: true })
                .then(stream => {
                    microphone = audioContext.createMediaStreamSource(stream);
                    microphone.connect(analyser);
                    analyser.connect(javascriptNode);

                    javascriptNode.onaudioprocess = () => {
                        const array = new Uint8Array(analyser.frequencyBinCount);
                        analyser.getByteFrequencyData(array);
                        const values = array.reduce((a, b) => a + b) / array.length;
                        drawVisualizer(values);
                        detectSpeech(values);
                    };

                    recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
                    recognition.continuous = true;
                    recognition.interimResults = true;

                    recognition.onresult = handleSpeechResult;
                    recognition.onend = () => {
                        if (isListening) recognition.start();
                    };
                    recognition.onerror = handleSpeechError;

                    recognition.start();
                    isListening = true;
                    responseDiv.textContent = "I'm listening...";
                })
                .catch(err => {
                    console.error('Error accessing microphone:', err);
                    responseDiv.textContent = "I'm having trouble hearing you. Could you check your microphone?";
                });
        }

        function detectSpeech(audioLevel) {
            const threshold = 30; // Adjust this value based on your needs
            if (audioLevel > threshold) {
                lastSpeechTimestamp = Date.now();
                if (isAssistantSpeaking) {
                    pauseAssistantSpeech();
                }
            } else if (Date.now() - lastSpeechTimestamp > 1500 && isAssistantSpeaking) {
                resumeAssistantSpeech();
            }
        }

        function pauseAssistantSpeech() {
            if (currentUtterance && speechSynthesis.speaking) {
                speechSynthesis.pause();
                animateMouth(false);
            }
        }

        function resumeAssistantSpeech() {
            if (currentUtterance && speechSynthesis.paused) {
                speechSynthesis.resume();
                animateMouth(true);
            }
        }

        function handleSpeechResult(event) {
            const userInput = Array.from(event.results)
                .map(result => result[0].transcript)
                .join('');
            
            if (event.results[0].isFinal) {
                if (userInput.toLowerCase().includes('goodbye')) {
                    stopListening();
                    respondToUser("Goodbye! It was nice talking to you.");
                    return;
                }
                fetchResponse(userInput);
            }
        }

        function handleSpeechError(event) {
            console.error('Speech recognition error:', event.error);
            responseDiv.textContent = "I didn't quite catch that. Could you please repeat?";
        }

        function stopListening() {
            if (microphone) {
                microphone.disconnect();
                analyser.disconnect();
                javascriptNode.disconnect();
                isListening = false;
                recognition.stop();
            }
        }

        function drawVisualizer(values) {
            visualizerContext.clearRect(0, 0, visualizer.width, visualizer.height);
            const height = values * visualizer.height / 255;
            const gradient = visualizerContext.createLinearGradient(0, 0, 0, visualizer.height);
            gradient.addColorStop(1, '#1a2a6c');
            gradient.addColorStop(0.5, '#b21f1f');
            gradient.addColorStop(0, '#fdbb2d');
            visualizerContext.fillStyle = gradient;
            visualizerContext.fillRect(0, visualizer.height - height, visualizer.width, height);
        }

        async function fetchResponse(userInput) {
            try {
                const response = await fetch('https://clean-rodent-stable.ngrok-free.app/ask2', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({ question: userInput, user_id: userId })
                });

                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }

                const data = await response.json();
                if (data && data.answer) {
                    respondToUser(data.answer);
                } else {
                    responseDiv.textContent = "I'm not sure how to respond to that. Could you rephrase?";
                }
            } catch (error) {
                console.error('Error fetching response:', error);
                responseDiv.textContent = "I'm having trouble thinking of a response. Give me a moment.";
            }
        }

        function respondToUser(text) {
            const sentences = text.match(/[^\.!\?]+[\.!\?]+/g) || [text];
            let currentSentence = 0;

            function speakNextSentence() {
                if (currentSentence < sentences.length) {
                    const utterance = new SpeechSynthesisUtterance(sentences[currentSentence]);
                    currentUtterance = utterance;

                    utterance.voice = speechSynthesis.getVoices().find(voice => voice.name === 'Google UK English Male') || speechSynthesis.getVoices()[0];
                    utterance.pitch = 1 + (Math.random() * 0.2 - 0.1); // Slight pitch variation
                    utterance.rate = 1 + (Math.random() * 0.2 - 0.1);  // Slight rate variation

                    utterance.onstart = () => {
                        isAssistantSpeaking = true;
                        animateMouth(true);
                    };

                    utterance.onend = () => {
                        isAssistantSpeaking = false;
                        animateMouth(false);
                        currentSentence++;
                        setTimeout(speakNextSentence, 500); // Pause between sentences
                    };

                    utterance.onerror = (event) => {
                        console.error('SpeechSynthesis error:', event);
                    };

                    speechSynthesis.speak(utterance);
                } else {
                    isAssistantSpeaking = false;
                }
            }

            responseDiv.textContent = text;
            speakNextSentence();
        }

        function resetInactivityTimeout() {
            if (inactivityTimeout) clearTimeout(inactivityTimeout);
            inactivityTimeout = setTimeout(() => {
                if (isListening && !isAssistantSpeaking) {
                    respondToUser("Is everything okay? I'm still here if you need me.");
                }
            }, 30000); // 30 seconds of inactivity
        }

        startListening();
        animateAvatar();
        resetInactivityTimeout();
    });
    </script>
</body>
</html>
